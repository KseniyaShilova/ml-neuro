{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c30ba64-a461-4c53-8ba7-9714e8528344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score\n",
    "#from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim.downloader as api\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116f54d7-1f14-462c-875b-1ae25a227667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                  MNTA Over 12.00            1\n",
       "4                                   OI  Over 21.37            1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Desktop\\stock_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a3ef1a-d80b-4f12-9e86-c6deefc3ece3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5791, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d461ab3-8049-4ce5-9c7b-19d6e5a5651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5791 entries, 0 to 5790\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       5791 non-null   object\n",
      " 1   Sentiment  5791 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 90.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acea08e1-a546-4946-aada-ffe5466571be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[\"Text\"].sample().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8491885f-17f9-4aae-b08c-9f53e23ed3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd786c7a-d0e5-4e96-878e-516d44aa6846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>ove GNW!! 9-13 Calls are making me feel better...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>CSN option trader buys 1,500 of the Jan 11-16 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>people slag AAP for cannibalization but samsun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5773</th>\n",
       "      <td>Sensex opens 166 points lower at 35,469, Nifty...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>CSOD Conf Call: CEO: feeling good about our po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "4201  ove GNW!! 9-13 Calls are making me feel better...          1\n",
       "387   CSN option trader buys 1,500 of the Jan 11-16 ...          1\n",
       "4385  people slag AAP for cannibalization but samsun...          1\n",
       "5773  Sensex opens 166 points lower at 35,469, Nifty...         -1\n",
       "2348  CSOD Conf Call: CEO: feeling good about our po...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ff0f52-f884-4d96-a720-09d145598fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>MCP take-over chatter... (I know don't laugh...)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>AMZN 1,200 lot bid in the Feb weekly 255P. 27 ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>KO made with sugar is sold at local COST. Cons...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>HNZ Another American Institution sold to forei...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Will watch the close carefully then decide whe...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "1891   MCP take-over chatter... (I know don't laugh...)         -1\n",
       "1550  AMZN 1,200 lot bid in the Feb weekly 255P. 27 ...         -1\n",
       "1049  KO made with sugar is sold at local COST. Cons...          1\n",
       "2523  HNZ Another American Institution sold to forei...         -1\n",
       "156   Will watch the close carefully then decide whe...         -1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccdd7aed-a359-4fc1-9dcf-648a42d3cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = SnowballStemmer(\"english\")\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79bedd9c-086a-419c-be32-8d63848748b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Earnings'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(\"Earnings\")\n",
    "lemma.lemmatize(\"Earnings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fcbdd0-c012-464b-8d02-9c7cfb036259",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae63f67-515b-48ad-8718-fb37045e3656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700b9358-8e46-453e-a69e-029f1f2ff0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove(\"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd250066-03ca-41d6-8c6d-1f178b20d256",
   "metadata": {},
   "source": [
    "def clean_text(text):\n",
    "    # приводим текст к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # создаем регулярное выражение для удаления лишних символов\n",
    "    regular = r'[\\*+\\#+\\№\\\"\\-+\\+\\=+\\?+\\&\\^\\.+\\;\\,+\\>+\\(\\)\\!\\@\\/+\\:\\\\+]'\n",
    "    # регулярное выражение для замены ссылки на \"URL\"\n",
    "    regular_url = r'(http\\S+)|(www\\S+)|([\\w\\d]+www\\S+)|([\\w\\d]+http\\S+)'\n",
    "    # удаляем лишние символы\n",
    "    text = re.sub(regular, '', text)\n",
    "    # заменяем ссылки на \"URL\"\n",
    "    text = re.sub(regular_url, r'URL', text)\n",
    "    # заменяем числа и цифры на ' NUM '\n",
    "    text = re.sub(r'(\\d+\\s\\d+)|(\\d+)','NUM', text)\n",
    "    # удаляем лишние пробелы\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # возвращаем очищенные данные\n",
    "    return text\n",
    " \n",
    "# создаем список для хранения очищенных данных\n",
    "cleaned_text = []\n",
    "# для каждого сообщения text из столбца data['Message']\n",
    "for text in df['Text']:\n",
    "    # очищаем данные  \n",
    "    text = clean_text(text)\n",
    "    # добавляем очищенные данные в список cleaned_text\n",
    "    cleaned_text.append(text)\n",
    "# записываем очищенные данные в новую колонку 'Cleaned_msg'\n",
    "#df['Cleaned_msg'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a0b5e2d-b939-4bb6-9755-97b0a584cac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing(token):\n",
    "    # Copied from here\n",
    "    token = re.sub(r\"(...)our$\", r\"\\1or\", token)\n",
    "    token = re.sub(r\"([bt])re$\", r\"\\1er\", token)\n",
    "    token = re.sub(r\"([iy])s(e$|ing|ation)\", r\"\\1z\\2\", token)\n",
    "    token = re.sub(r\"ogue$\", \"og\", token)\n",
    "    return token\n",
    "def correct_typo(tokens):\n",
    "    spell = SpellChecker()\n",
    "    return [spell.correction(t) if len(spell.unknown([t]))>0 else t for t in tokens]\n",
    "        \n",
    "def preprocess_text(text):\n",
    "    # 1. Tokenise to alphabetic tokens\n",
    "    tokeniser = RegexpTokenizer(r'[A-Za-z]+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    # 2. Lowercase and lemmatise\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    tokens = [lemmatiser.lemmatize(t.lower(), pos='v') for t in tokens]\n",
    "\n",
    "    tokens = correct_typo(tokens)\n",
    "    \n",
    "    # 4. Convert British spelling to American spelling (this won't convert 100%)\n",
    "    tokens = [convert_to_american(t) for t in tokens]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['cannot', 'could', 'done', 'let', 'may' 'mayn',  'might',  'must', 'need', 'ought', 'oughtn', \n",
    "                       'shall', 'would', 'br'])\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a419f33-76fc-4fba-964f-5743589b3d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2d75d33-f6ab-4c25-8841-5983bcf33ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(train_df[\"Text\"].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b2da75-ff75-4bdd-89f7-d59d63a95e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SpellChecker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9356/3459696185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text_proceed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text_proceed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\mlisuct\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\mlisuct\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\mlisuct\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1100\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\mlisuct\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9356/544119107.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'v'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect_typo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# 4. Convert British spelling to American spelling (this won't convert 100%)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9356/544119107.py\u001b[0m in \u001b[0;36mcorrect_typo\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcorrect_typo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mspell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SpellChecker' is not defined"
     ]
    }
   ],
   "source": [
    "train_df[\"Text_proceed\"] = train_df[\"Text\"].apply(preprocess_text)\n",
    "test_df[\"Text_proceed\"] = test_df[\"Text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c22c20-1e7e-430d-9d47-33f57cee09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Text_proceed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51beff7e-b004-4053-80d8-facef77633a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_FIT = \"Text_proceed\"\n",
    "TARGET_COL = \"Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42e950-506c-4a06-ad51-a96821f11fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def get_mean_vector(self, text):\n",
    "        v = np.zeros(300)\n",
    "        c = 0\n",
    "        for word in text.split(\" \"):\n",
    "            if word in self.model:\n",
    "                v += self.model.get_vector(word)\n",
    "                c += 1\n",
    "        c = max(43, c)\n",
    "        return v / c\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([self.get_mean_vector(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0001e-b568-4d32-8e73-fcef8d84d1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719718d-e189-41b6-92a0-c40d60f3c9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8eabe6-3c29-4969-8d86-fb54d28a1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 1), min_df = 18, max_df=0.1)),\n",
    "    (\"model\", LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c358a5-e970-4d3c-a06a-81ffc65b7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_df, test_df):\n",
    "    model.fit(train_df[COLS_TO_FIT], train_df[TARGET_COL])\n",
    "    pr = model.predict(test_df[COLS_TO_FIT])\n",
    "    mse = mean_squared_error(test_df[TARGET_COL], pr)\n",
    "    out = accuracy_score(test_df[TARGET_COL], pr)\n",
    "    print(f\"mse: {mean_squared_error(test_df[TARGET_COL], pr):.3f}\")\n",
    "    print(f\"mae: {mean_absolute_error(test_df[TARGET_COL], pr):.3f}\")\n",
    "    print(f\"out: {accuracy_score(test_df[TARGET_COL], pr):.3f}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246da552-fca3-4f05-8ef5-280301d09f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697e054-eada-4580-86d8-39f31682580b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821f091-738f-412c-994c-caa9a2ad1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer()),\n",
    "    (\"model\", LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a633000-793b-435b-a55b-4e18c1f54783",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0a056-503a-44d3-a76d-e749a1e9e0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f88e9-d4e1-4105-a6ed-5383e4bae138",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0a693-fde4-48a5-b852-f3fa1907ff01",
   "metadata": {},
   "source": [
    "# Лучший результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2314060-e0d4-4f44-9faa-7d59d22eaac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 9))),\n",
    "    (\"model\", LogisticRegression(C=0.91, penalty=\"l1\", solver=\"saga\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feaf999-5e72-4766-bb3a-2486026a35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab693e1-d0de-4a1a-8fff-505da9533747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a58da-606a-4741-8b13-b9e0a8a26afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(token_pattern=r\"\\S+\", ngram_range=(1, 1))),\n",
    "    (\"model\", LogisticRegression(C=2.5, penalty=\"l2\", solver=\"saga\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bd1e2-ae3f-4df7-8fa6-ed24af05cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92559902-90a8-49b6-8a7d-0c644b028f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ff7ff-a671-4139-b49a-5c4e3caaef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 9), min_df = 5, max_df=0.1)),\n",
    "    (\"model\", LogisticRegression(C=2, penalty=\"elasticnet\", solver=\"saga\", l1_ratio=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7486be-869a-49db-a857-2ca6f3f8badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb334c-9ea9-4636-915d-0e9f315b153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer()),\n",
    "    (\"model\", LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99486c43-6a9f-4070-a0f0-6624640d175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ea6f5-199f-4f36-9cf2-725537e5a742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "883e580b-086a-4576-8452-63cb371cac8e",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "    'model__penalty': [\"l1\"],\n",
    "    'vectorizer__ngram_range': [(1,9)],\n",
    "   # 'vectorizer__max_df': np.arange(0.1, 0.9, 0.1),\n",
    "    #'vectorizer__min_df': np.arange(1, 10, 1),\n",
    "    #'model__l1_ratio' : [0.1],\n",
    "    'model__solver': [\"saga\"],\n",
    "    'model__C': np.arange(2.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e189ab2-59c4-4371-aa32-099d0bcb4bcf",
   "metadata": {},
   "source": [
    "cv = GridSearchCV(estimator=pipe, param_grid=parameters, cv=KFold(5, shuffle=True), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8fb89-142f-4066-abb2-448b59d1c41a",
   "metadata": {},
   "source": [
    "cv.fit(train_df[COLS_TO_FIT], train_df[TARGET_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89aa8d6-8e15-455c-99e3-930cb111ec57",
   "metadata": {},
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf9231-817f-4a07-9a21-5b8643ae3e73",
   "metadata": {},
   "source": [
    "cv.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ef387-a93f-4ae5-919e-850f2a19d56d",
   "metadata": {},
   "source": [
    "cv.best_estimator_.named_steps[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4ee74-bd4c-42da-8f6e-6a3dd290ee5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e37f60d0-9eb4-4ba5-9e7d-363180996d4f",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "    'model__penalty': [\"l1\"],\n",
    "    'vectorizer__ngram_range': [(1,9)],\n",
    "    'vectorizer__max_df': [(0.1)],\n",
    "    'vectorizer__min_df': [(1)],\n",
    "    #'model__l1_ratio' : [0.1],\n",
    "    'model__solver': [\"saga\"],\n",
    "    'model__C': np.arange(0.01, 1, 0.01)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032677b-8e54-4a92-98c3-596354da5332",
   "metadata": {},
   "source": [
    "cv = GridSearchCV(estimator=pipe, param_grid=parameters, cv=KFold(5, shuffle=True), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed6e2a-f511-4974-bd37-c37ceb70b347",
   "metadata": {},
   "source": [
    "cv.fit(train_df[COLS_TO_FIT], train_df[TARGET_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810beecf-663a-4bef-82f3-1ddfaab911d4",
   "metadata": {},
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23df84-7188-4399-b633-64205dbc7e8f",
   "metadata": {},
   "source": [
    "cv.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840fc43-3c68-4aa1-a353-0cb6c2669dea",
   "metadata": {},
   "source": [
    "cv.best_estimator_.named_steps[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375ab63-9f65-4d73-824c-5d35125d9abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3340e08a-ce31-427d-b246-8e17a46d7cb9",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "    #'model__penalty': [\"l1\"],\n",
    "    #'vectorizer__ngram_range': [(1,9)],\n",
    "    #'vectorizer__max_df': [(0.1)],\n",
    "    #'vectorizer__min_df': [(1)],\n",
    "    #'model__l1_ratio' : [0.1],\n",
    "    #'model__solver': [\"saga\"],\n",
    "    'model__C': np.arange(1, 10, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3a341-c382-469d-afa7-79ee45f10a1f",
   "metadata": {},
   "source": [
    "cv = GridSearchCV(estimator=pipe, param_grid=parameters, cv=KFold(5, shuffle=True), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb2caf-0bfa-4d07-bfd6-b1d53d3a290f",
   "metadata": {},
   "source": [
    "cv.fit(train_df[COLS_TO_FIT], train_df[TARGET_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb10cc-c072-4b02-8cb9-06b518742188",
   "metadata": {},
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa6e60-5ded-4cf9-9c56-7db87e4dfd04",
   "metadata": {},
   "source": [
    "cv.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22421e00-35b4-42be-9c60-be083467130c",
   "metadata": {},
   "source": [
    "cv.best_estimator_.named_steps[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76f892-da44-4c02-b9f4-7f24364ab772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdb16a-b2a2-4fbb-8c37-a6a437909b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    \n",
    "    \n",
    "    (\"vectorizer\", CountVectorizer()),\n",
    "    (\"model\", LogisticRegression(solver=\"saga\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb653af3-a8a5-4007-9c3e-6d213a659f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c0cf9-c5a9-47f5-ab6c-b9baf493f292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40796430-0def-4350-adcd-f32aba98c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 9), min_df = 1, max_df=0.1)),\n",
    "    (\"model\", LogisticRegression(C=0.91, penalty=\"l1\", solver=\"saga\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f84885-d13e-4bc3-a0e1-611c9208950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bde80d-c3fc-4ec4-aaca-f842303fe369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd6388-c8f4-4f0c-86b7-2898f58e8f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc8edc-dd60-4306-99ee-e3418048ef86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
