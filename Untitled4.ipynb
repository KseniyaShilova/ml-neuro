{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d336a93-7fd9-408d-9a87-0def019fb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, SnowballStemmer, RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdcfc231-c749-46d0-9708-03582403cc37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                  MNTA Over 12.00            1\n",
       "4                                   OI  Over 21.37            1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Desktop\\stock_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64eabdfa-8a3d-43dd-afb2-980bc8c2c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594aae1e-9559-4126-8baf-993026bbe7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd978bfb-f11f-47c9-85f9-6323fee5c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = SnowballStemmer(\"english\")\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a910930c-cf13-46ac-9366-01374c4ff7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed20d705-f853-4483-a54b-8a368e2429f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = set(stopwords_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b50bc0-a444-4877-8bd4-97b0a37796d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94fa8cbf-e92b-42d8-88f2-28068d1447be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3268b8dc-bfab-4e79-a4e1-d18bfe267625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для очистки текста\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    regular = r'[\\*+\\#+\\№\\\"\\-+\\+\\=+\\?+\\&\\^\\.+\\;\\,+\\>+\\(\\)\\/+\\:\\\\+]'\n",
    "    regular_url = r'(http\\S+)|(www\\S+)|([\\w\\d]+www\\S+)|([\\w\\d]+http\\S+)'\n",
    "    text = re.sub(regular, '', text)\n",
    "    text = re.sub(regular_url, r'URL', text)\n",
    "    text = re.sub(r'(\\d+\\s\\d+)|(\\d+)',' NUM ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "  \n",
    "cleaned_text = []\n",
    "# создаем список для хранения токенов\n",
    "tokens = []\n",
    " \n",
    "# для каждого сообщения text из столбца data['Message']\n",
    "for text in df['Text']:\n",
    "    # очищаем данные и сохраняем результат в списке cleaned_text  \n",
    "    text = clean_text(text)\n",
    "    cleaned_text.append(text)\n",
    "     \n",
    "    #разбиваем текст на токены с сохраняем результат в списке tokens\n",
    "    text = word_tokenize(text)\n",
    "    tokens.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34fa81e4-91ac-42a8-bc5a-ac47576aa62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Text_proceed\"] = train_df[\"Text\"].apply(clean_text)\n",
    "test_df[\"Text_proceed\"] = test_df[\"Text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "192202f7-4a82-4f70-9003-236a0045828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_FIT = \"Text_proceed\"\n",
    "TARGET_COL = \"Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591c1ee-2d29-4444-a881-035d47523270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413b1d0-ec2e-4e65-a71c-e43d82a5d704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec7815c-6c5f-4d2e-ac43-4602fb03458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_df, test_df):\n",
    "    model.fit(train_df[COLS_TO_FIT], train_df[TARGET_COL])\n",
    "    forecast = model.predict(test_df[COLS_TO_FIT])\n",
    "    answer = accuracy_score(test_df[TARGET_COL], forecast)\n",
    "    print(f\"answer: {accuracy_score(test_df[TARGET_COL], forecast):.3f}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d2ae1e-2397-4e9d-b473-b5ccdc58971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ac637e-ea8f-409e-a7e6-b8f194c2954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def get_mean_vector(self, text):\n",
    "        v = np.zeros(300)\n",
    "        c = 0\n",
    "        for word in text.split(\" \"):\n",
    "            if word in self.model:\n",
    "                v += self.model.get_vector(word)\n",
    "                c += 1\n",
    "        c = max(43, c)\n",
    "        return v / c\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([self.get_mean_vector(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8052545-9ef7-4715-94eb-fe2d66491d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c60d976a-d695-44a4-8eb4-d366402163bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 2))),\n",
    "    (\"model\", LogisticRegression(solver=\"saga\", C=5.7, penalty=\"l1\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9863fd63-5a23-4981-981f-4da83f57adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: 0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\mlisuct\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8127696289905091"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61af2d08-aec1-48e6-9e51-34f0cc0fe73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(token_pattern=r\"\\S+\", ngram_range=(1, 1))),\n",
    "    (\"model\", LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", l1_ratio=1, C= 5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bbf5eab-5786-47fa-a09b-c4301f3c5007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\mlisuct\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8015530629853321"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed48705-4f40-4f47-91aa-65c4ec399680",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 2))),\n",
    "    (\"model\", LogisticRegression(solver=\"saga\", C=5.89, penalty=\"l2\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfb90c65-288e-4090-9de5-dcbdef9a46ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\mlisuct\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8015530629853321"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_validate(pipe, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07223c5-58d8-4472-8528-5ddbae70c15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5763940-ad59-4e7a-9448-6572858687c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
